% 
% Annual Cognitive Science Conference
% Sample LaTeX Paper -- Proceedings Format
% 

% Original : Ashwin Ram (ashwin@cc.gatech.edu)       04/01/1994
% Modified : Johanna Moore (jmoore@cs.pitt.edu)      03/17/1995
% Modified : David Noelle (noelle@ucsd.edu)          03/15/1996
% Modified : Pat Langley (langley@cs.stanford.edu)   01/26/1997
% Latex2e corrections by Ramin Charles Nakisa        01/28/1997 
% Modified : Tina Eliassi-Rad (eliassi@cs.wisc.edu)  01/31/1998
% Modified : Trisha Yannuzzi (trisha@ircs.upenn.edu) 12/28/1999 (in process)
% Modified : Mary Ellen Foster (M.E.Foster@ed.ac.uk) 12/11/2000
% Modified : Ken Forbus                              01/23/2004
% Modified : Eli M. Silk (esilk@pitt.edu)            05/24/2005
% Modified : Niels Taatgen (taatgen@cmu.edu)         10/24/2006
% Modified : David Noelle (dnoelle@ucmerced.edu)     11/19/2014
% Modified : Roger Levy (rplevy@mit.edu)     12/31/2018

%% Change "letterpaper" in the following line to "a4paper" if you must.

\documentclass{llncs}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
%\cogscifinalcopy % Uncomment this line for the final submission 

\usepackage{hyperref}
%\hypersetup{colorlinks=true,citecolor=blue}
%\usepackage{pslatex}
%\usepackage{apacite}
\usepackage{float} % Roger Levy added this and changed figure/table
                   % placement to [H] for conformity to Word template,
                   % though floating tables and figures to top is
                   % still generally recommended!

%\usepackage[none]{hyphenat} % Sometimes it can be useful to turn off
%hyphenation for purposes such as spell checking of the resulting
%PDF.  Uncomment this block to turn off hyphenation.

%\setlength\titlebox{4.5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 4.5cm (the original size).
%%If you do, we reserve the right to require you to change it back in
%%the camera-ready version, which could interfere with the timely
%%appearance of your paper in the Proceedings.

%%% Comments
\newcommand{\eat}[1]{}
\newcommand{\xxcomment}[4]{\textcolor{#1}{[$^{\textsc{#2}}_{\textsc{#3}}$ #4]}}
\usepackage[dvipsnames]{xcolor}
\newcommand{\rh}[1]{\xxcomment{orange}{R}{H}{#1}}
\newcommand*{\img}[1]{%
    \raisebox{-.3\baselineskip}{%
        \includegraphics[
        height=\baselineskip,
        width=\baselineskip,
        keepaspectratio,
        ]{#1}%
    }%
}


\newcommand{\rabbit}[1]{\textrm{\img{figures/rabbit.png}}}
\newcommand{\squirrel}[1]{\img{figures/squirrel.png}}

\title{Distributed statistical inference in social interaction networks under conditions of epistemic difficulty} % depends on the communication modality
\author{Author Name\inst{1} \and Author Name\inst{2}}
\institute{Affiliation 1, Email \email{name1@example.com} \\
           Affiliation 2, Email \email{name2@example.com}}


\begin{document}
\maketitle
\begin{abstract}
Social learning comes with significant epistemic risks: information may be inflated or ignored as it spreads, reducing the accuracy of collective knowledge. 
How do networks of learners overcome these challenges to successfully aggregate private information and to arrive at accurate collective beliefs?
Here, we examine the role of the communication channel, asking whether this process is better served by direct belief transmission or by richer forms of natural-language communication.
The latter allows for discussion of the underlying evidence but also may create additional risks due to the noise and imprecision associated with natural language. 
In a series of studies, we asked groups ($N = 564$) to estimate an underlying probability distribution in a round-robin dyadic paired network of 4. 
 We varied the information quality on three factors: total size of group dataset, how well the information represented the underlying probability (representativeness), and how fairly the information was distributed among the players (entropy). 
We found that all groups were able to use social information to converge toward more accurate inferences. 
Yet groups in the belief transmission condition perform more poorly on average. Language facilitates aggregation of information across networks. We also found interaction between condition and representativeness. Unrepresentative private information is harder for groups that do not have access to natural language, and the condensing of previously collected group information in peer to peer communication is critical in this process. Models of collective behavior must move beyond direct belief transmission, to capture the epistemic value of language.

\textbf{Keywords:} 
Social learning; statistical inference; collective behavior
\end{abstract}


\section{Introduction}

Humans rely on social learning to go beyond their personal experience with the world, gaining knowledge that could not be collected by one individual alone \cite{Hahn2023Dependence}. 
However, social learning comes with epistemic risks: information may be inflated or ignored as it spreads, damaging the accuracy of collective knowledge. 
How do networks of learners overcome these challenges to successfully aggregate private information and to arrive at accurate collective beliefs?
Here, we examine the role of the communication channel, asking whether this process is better served by belief reports (i.e. individuals directly transmitting their current belief state) or by richer forms of natural-language communication that allow for discussion of the underlying evidence but also may create additional risks due to the noise and imprecision associated with natural language. 

Most agent-based models of social learning and belief revision assume some form of direct information transmission between agents \cite{volzhanin2015individual, Tang2023CollectiveIntelligence, smaldino2023modeling} treating communication as copying or averaging another agent's belief states. 
Direct belief transmission through numerical estimates has clear epistemic advantages: it is precise, efficient, and reduces communication to the essential information needed for belief updating.
On the other hand, natural language communication also has key advantages, enabling more sophisticated information sharing strategies by allowing individuals to explain their reasoning, highlight uncertainties, and negotiate different interpretations of evidence.

We hypothesized that the relative effectiveness of these communication modalities depends on the epistemic conditions of the inference problem. 
When evidence is abundant and evenly distributed across individuals, simple belief sharing may be sufficient for coordination. 
However, when evidence is sparse, unevenly distributed, or unrepresentative, the ability to discuss the raw data or reflect on conflicting information received from different partners through natural language may become crucial. 
Here we provide an empirical examination of how different communication modalities affect the ability of groups to aggregate distributed information under varying conditions of epistemic difficulty.

In our study, $N=2256$ participants from Prolific were connected into $N=564$ networks of size four for a collective inference task. 
They were asked to estimate the relative proportion of rabbits vs. squirrels in the local wildlife population given only a fixed set of critters they could see out their virtual `window' as initial private information to be shared in subsequent rounds of social interaction.
Each group was either assigned to the \emph{belief report} condition, where they were given a slider to communicate their beliefs, or the \emph{natural language} condition, where they were given an interactive chat box to communicate with other participants. In the initial version of this experiment, there was also a \textit{unidirectional natural language} condition.
On each round of the task, participants were paired with one other participant in the group and given 30 seconds to communicate their beliefs on a slider (in the \emph{belief report} condition) or to discuss their beliefs about the underlying distribution with their partner (\emph{natural language} condition).
Dyadic pairings on each trial were rotated using a round-robin algorithm such that participants were paired with each partner exactly two times over the course of eight trials.
We varied the difficulty of the task along three continuous dimensions.
First, groups were given a smaller or larger total number of observations (i.e. differential sample size). 
Second, some groups had all of the observations concentrated in the backyard of a single participant while in others the observations were evenly distributed (i.e. differential entropy).
Third, for some groups, each person's private information was close to the true distribution and for others each individual had more skewed distributions (i.e. 'representativeness' of the private information).


\begin{figure*}
    \centering
    \includegraphics[scale=.45]{figures/method3.pdf}
    \caption{Participants were connected in groups of four and observed a private sample of wildlife (here, one squirrel and two rabbits). On each trial of the experiment, they were asked to write a message to a neighbor and report their beliefs about the underlying distribution after receiving a message. }
    \label{fig:methods}
\end{figure*}

Controlled behavioral experiments are critical for revealing divergences from idealized theoretical dynamics. 
In this paper, we report results from a novel group inference task where participants exchanged messages in small social networks under varied conditions. By tracking how beliefs evolved over repeated interactions, we directly measured the pathways by which social information shapes collective knowledge. Our findings reveal that subtle factors in communication mechanics and network structure can accelerate or impair convergence to accurate beliefs. For example, introducing a constrained numerical format for messaging akin to classical models actually slowed or reversed the ability of the group to converge on the true value. These effects demonstrate the need to move beyond simplistic models of direct transmission and incorporate richer cognitive mechanisms of information exchange. More broadly, our work aims to provide a stronger empirical foundation for a new generation of models that capture the complexities of collective human inference.
One hypothesis is that people behave in a quasi-Bayesian manner, weighting social and private signals proportional to their relative uncertainty or reliability. This mechanism hypothetically allows groups to combine diverse information sources, marking value through selective summarization, to make optimal collective inferences. 

\section{Experiment 1: The effect of the communication medium}

Groups effectively aggregate private knowledge into more accurate collective estimates by exchanging information.\footnote{This experiment is based on our previous work in the 2024 CogSci proceedings. This version was fixed to 20 critters to control the distribution between games. As we found preliminary evidence of epistemic complexity being a factor, we also fixed it to 20 critters to separate out the epistemic complexity from the communication medium.  %(and preregistered but only the first half :( )
} %cite myself%
However, it remains unclear how the specific medium of communication impacts this process. 
Classic agent-based models \cite<e.g.>[]{DeGroot1974:Reaching-a-Cons} typically assume that agents have direct access to the underlying beliefs of neighboring agents.
On one hand, we may expect the linguistic bottleneck that free-form messages must pass through may reduce the efficiency and accuracy of opinion transmission compared to directly sharing numerical representations, thus impairing convergence. 
On the other hand, the flexibility and richness of natural language may allow for new coordination strategies that improve on simplistic averaging models. 

By comparing what makes groups more or less effective across these conditions, we aim to reveal how the \emph{communication modality} of social networks shape their collective outcomes \cite{boyce2022two}.

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.65]{placeholder_figures/error.png}
    \includegraphics[scale=0.65]{placeholder_figures/prev_guess.png}

    \caption{(left) Among our three conditions, groups using unidirectional messaging performed best (Experiment 1), groups using slider-only communication performed worst, and groups using interactive channels were in between (Experiment 2). (right) Participants in all three conditions change their reported beliefs less and less over the course of the study.}
    \label{fig:channel}
\end{figure*}


\subsubsection{Participants}

We recruited $N=948$ participants from Prolific and assigned them to $N=237$ groups of four. 
All participants were from the US, UK, or Canada, and were pre-screened as fluent English speakers.
Active participants received a base pay of \$15 US per hour, and the experiment took approximately 15 minutes.
We excluded participants who failed to provide responses for 2 consecutive trials. All models were run with the corresponding social networks both excluded and included as a robustness test. 

\subsubsection {Stimuli}

Participants were asked to estimate the relative proportion of rabbits \rabbit{} vs. squirrels \squirrel{} in the local wildlife population.
We showed each participant a fixed set of critters out their virtual ``window'' as initial private information to be shared in subsequent rounds of social interaction.
Private samples of critters for each participant in a given network were drawn from a fixed sample with a fixed proportion $p$. 
\begin{equation*}
P(\rabbit{} \mid p) = {20 \choose \rabbit{}} p^{\rabbit{}} (1 - p)^{20 - \rabbit{}}
\end{equation*}

In other words, for each participant in a network, we first fixed 20 to be the total number of observed critters, and then we sampled their group observations from a fixed binomial distribution of probability $p$ and assigned them randomly across participants, with a minimum of 2 critters per participant. 
This procedure allowed for variability in the local proportions, while maintaining a constant underlying probability with a fixed amount of total information across the entire network.
We ran participants in two conditions, setting $p= 0.7$ for half of the networks and $p=0.3$ for the other half. 

\subsubsection{Procedure}

After reading the task instructions and passing a comprehension quiz, participants were directed to a reactive web application built with Empirica \cite{almaatouq2021empirica}.
The task interface had two major components.
On the left side of the screen, a group of critters (rabbits and squirrels) were presented out a `window'.
On the right side of the screen, a messaging interface was provided to communicate with other participants in the network and report beliefs about the true underlying distribution (see \autoref{fig:methods}). 
We manipulated this communication interface across three distinct conditions (see Experiment 2).
%Users were incentivized to come to a group consensus. 

On each round of the task, participants were paired with another participant in the group and given 30 seconds to send a message or messages about the underlying distribution. 

Three types of communication modality were tested.
In the \emph{unidirectional} modality, there was no limit to the number of messages they could send, but they did not receive any messages back during this period. The \emph{interactive} modality used the same two-stage control flow, but instead of waiting until the report stage to receive messages, dyads were able to bidirectionally exchange messages in real time for the entire 30 second communication window, similar to a one-on-one text messaging conversation. 

Following the communication stage, they were shown the message(s) produced by their partner and asked to report their own updated beliefs about the underlying distribution using a slider. 
The slider ranged from no rabbits (100\%~\squirrel{}) to all rabbits (100\%~\rabbit{}). 
Participants proceeded to the next round after 30 seconds or as soon as all members of their network had submitted their responses. 
Dyadic pairings on each trial were rotated using a round-robin algorithm such that participants sent messages to, and received messages from, each partner exactly three times over the course of nine trials. A round robin system was implemented to increase the influence of the total network of beliefs on any one interaction, placing emphasis on collective information validation over n = 9 rounds rather than cumulative information sharing over n = 4 peer interactions.
Demographic information was collected in an exit survey following the final round.

The third modality, '\emph{slider-only}' constructed a close behavioral replica of classic agent-based models. 
Communication entirely took place through direct numerical data transmission of opinion reports. 
Rather than starting with a message-sending phase and then proceeding to a belief report phase, individuals were initially prompted to input their opinion using a slider.
After all participants submitted their belief reports, they advanced directly to the next round, where they were shown the same slider interface but with an auto-generated message (e.g. ``I think the population is 73\% rabbits''). 
In other words, the two stages were merged, because providing a belief report is the same act as providing a message. 

\subsection{Results} 
% stringent filtering - excluded games with insufficent players INCLUDING games with inattentive players
\subsubsection{Communication improves inferential accuracy}
We begin by considering the extent to which groups are able to improve their collective accuracy through deliberation. 
We hypothesized that, although each participant received sparse local observations, the network as a whole would be able to aggregate their estimates over time to approach the true latent probability.
We tested this hypothesis using a linear mixed-effects model predicting each slider rating (0 to 100) including fixed effects of round index (continuous; 1 to 9), latent probability condition (sum-coded; 0.3 vs. 0.7), and their interaction.

Due to nested layers of clustered variation, we included intercepts and random effects of round index for each player and game. 
We found a significant interaction,$b=0.69$, $t(7265.0)=8.53$, $p<0.001$, %model <- lmer(guess ~ treatment * idx + (1 | gameID/playerID), data = d.guesses)  reported treatment0.7:idx. 1+idx failed to converge
where participants in the $p=0.7$ condition increased their guesses over time while participants in the $p=0.3$ condition gradually decreased their guesses.
In other words, participants tended to regularize their initial guesses closer to the midpoint of the scale before moving in opposite directions as social information was acquired (see \autoref{fig:experiment1}, left). 

Next, to more directly examine our hypothesis that a game's error would decrease over time as messages are exchanged, we constructed a second mixed-effects model instead predicting trial-level \emph{error}: how far off each slider estimate was from the true probability.
As predicted, we found a significant main effect of round index,$b=-0.17$, $t(288.9)=-2.57$, $p=0.011$,%model2 <- lmer((error) ~ idx + (1+idx | gameID/playerID), data = error_data)
suggesting that error decreases and estimates converge toward the empirical game-level frequency across both conditions (see \autoref{fig:experiment1}, right).
These findings were robust to additional exclusion criteria, such as %removing players who happened to draw an empty sample (no critters at all), or <- didn't test this yet. not sure I have enough data w the existing exclusion criteria
removing games where more than one player was missing data due to inattention.% <-default state of data now

To test whether total information impacted convergence differently when accounting for communication modality, we compared a mixed-effects model including main effects of round index against a model including both condition and sample size. The model with an interaction provided a  better fit in a nested model comparison,$\chi^2(2)=7.57$, $p=0.023$, suggesting that the relationship between round stage and collective error depends on the communication medium.   % lmer((error) ~ idx + (1+idx | gameID/playerID), data = error_data)
%model4 <- lmer((error) ~ idx + condition + (1+idx |gameID/playerID), data = error_data)
%anova(model2, model4)

\subsubsection{Direct transmission of beliefs impairs accuracy improvement}

To evaluate the extent to which linguistic communication modalities improve or impair the distributed inference abilities of participants in our task, we compared the average error of each guess across conditions.
As in Experiment 1, we calculated error as the distance between the slider value and the network's empirical frequency of rabbits $\hat{p}_i$. 
We constructed a linear mixed-effects model with fixed effects of condition (unidirectional vs. slider-only vs. interactive, sum-coded) and round index (1 through 9, centered), with nested random effects for each game and each player within that game. 

First, we found a significant main effect of round index where error decreased over time in all conditions, $b=-3.74$, $t(324.3)=-2.47$, $p=0.014$. %mutate(condition = fct_relevel(condition, 'slider', 'unidirectional', 'interactive')) %>%
%   { print(contrasts(.$condition)); . } %>%
%  mutate(error = abs(guess - mleEstimateGame)) %>%
%  lmer(scale(error) ~ poly(idx, 2) + condition + (1 + poly(idx, 1) || gameID/playerID),
 %      data = .,
%       contrasts = list(condition = contr.sum(3)), #each condition is being compared to all of them
%       control = lmerControl(optimizer = 'bobyqa')) 

  %poly(idx, 2)1 here. could be poly(idx,2)2 - slider throws it all way off
More centrally for our hypothesis, however, we also found a main effect of condition, with the interactive condition performing better than slider and unidirectional at all time points than the average across conditions $b=0.19$, $t(115.3)=2.77$, $p=0.007$. %same as above but condition 1 (slider * interactive) 
Thus, we find some support for the hypothesis that, rather than serving as a bottleneck and impeding the direct transmission of belief states, language may in fact facilitate more accurate aggregation of information across networks.

\begin{figure}[t]
    \centering   
    \vspace{-3em}
    \includegraphics[scale=0.85]{placeholder_figures/outlier.png}
    \vspace{-3em}
    \caption{Participants whose private observations deviate more from the ground-truth proportions tend to adapt their guesses the most, reaching an error rate closer to participants with high information quality by round 12.}
        \vspace{-1em}
    \label{fig:exp1-outliers}
\end{figure}

\begin{figure*}[t!]
    \centering
    \includegraphics[scale=.5]{figures/treatments.pdf}
        \vspace{-2em}
    \caption{We compare three different communication modalities to examine their impact on group opinion dynamics. (A) In Experiment 1, we used a \emph{unidirectional} interface where messages were asynchronously sent and received. (B) In Experiment 2, we added a 'slider-only' condition, which constrains input to a proportion slider, generating a pre-worded message, as well as an 'interactive' condition allowing unconstrained dyadic communication through the chat box. }
    \label{fig:methods2}
\end{figure*}

\subsubsection{Outliers make bigger belief revisions}
% significant when abs
Next, we turned to examine how individual belief updates vary as a function of private and social information. 
It is commonly observed in the  collective behavior literature that agents make larger revisions to their estimates when they are more out of step with their neighbors \cite<e.g.>[]{becker2017network}. 
Here, we test the extent to which this effect replicates under linguistically-mediated communication. 
First, we calculated a measure of \emph{outlier degree} for each participant, defined as the distance between the empirical proportion of rabbits in a participant's private sample $\hat{p}_{ij}$ and the empirical proportion in their game as a whole $\hat{p}_i$. 
We then constructed a linear mixed-effects model predicting participants' error magnitudes as a function of their outlier degree, round index, and the interaction between the two. 
All variables were $z$-scored, and we included maximal game-level random effects\footnote{The estimated variance of the random interaction was near 0, so we removed it from the random effect structure.} 

As predicted from prior work, we found a significant interaction between outlier degree and round index on absolute error, $b=-0.66$, $t(1855.4)=-4.70$, $p<0.001$.  %d.MLE %>%  lmer(abs(error) ~ scale(outlierPercent) * scale(idx) + (1 + scale(outlierPercent) + scale(idx) | gameID),       data = .) %>%   summary() reported scale(outlierPercent):scale(idx)
Participants who happened to receive outlier samples (e.g. 2 rabbits and 1 squirrel in the condition where squirrels were more likely) initially had higher error in their estimates (light blue line) but the slope of this relationship significantly decreased over time (darker blue lines), indicating that these participants were revising their guesses without other participants' estimates getting significantly worse (see \autoref{fig:exp1-outliers}). 
In other words, participants receiving social information that deviates strongly from their own observations tend to most significantly revise their beliefs.


\subsubsection{Participants' guesses stabilize over time}

A final reason for the observed difference in performance across conditions is the speed of convergence. 
Intuitively, information obtained later during learning should have less of an impact on beliefs than information obtained at the beginning; however, it was unclear whether this would differ as a function of the communicative channel.
We measured the absolute difference between a player's guess on round $k$ and their guess on $k+1$ and constructed a mixed-effects model with fixed effects of round index and maximal random effect structure.
First, we found a significant main effect of round index, $b=-0.39$, $t(473.3)=-8.18$, $p<0.001$, indicating that participants were gradually stabilizing over time in all conditions; they were changing their responses less and less as the game went on.
Additionally, we found a significant main effect between the slider condition and the interactive condition, $b=1.86$, $t(180.8)=3.80$, $p<0.001$ where participants overall made much smaller changes in the slider-only condition.
%mutate(condition = fct_relevel(condition, 'slider', 'unidirectional', 'interactive')) %>%   { print(contrasts(.$condition)); . } %>%  lmer(    distToSelf ~ idx + condition + (1 + idx | gameID / playerID),    data = .,    contrasts = list(condition = contr.sum)

\section{Experiment 2: Effect of information quality}

In order to further explore the situations in which social communication is necessary, we created a set of epistemically difficult communication conditions. We varied the information quality on three factors: total size of group dataset, how well the information represented the underlying probability (representativeness), and how fairly the information was distributed among the players (entropy). To simplify the experiment, we only ran games with the underlying probability of 0.75. To test for slider directionality effects in the previous experiment, we flipped the slider for half of the experiment. \footnote{Appendix }

\begin{figure*}
    \centering
    \includegraphics[scale=.45]{placeholder_figures/explain difficulty.png}
   .
    \label{fig:methods}
\end{figure*}
\subsubsection{Stimuli}
We manipulated three axes of information distribution. We simulated 100,000 4-player draws based on the probability distribution used in our previous work. In order to vary the total amount of observations across participants, we placed a hyper-prior over the sample size $N$, yielding the following distribution:
\begin{equation*}
P(\rabbit{} | p) = \mathbb{E}_N \left[f(\rabbit{}, N, p)\right] = \mathbb{E}_N \left[{N\choose \rabbit{}} p^{\rabbit{}}(1-p)^{\squirrel{}}\right]
\end{equation*}
for $N\sim \textrm{Unif}\{0,35\}$. 
We then grouped the simulated data based on our three chosen axes:
\begin{itemize}
    \item  Total information of the group: total critters
    \item Between player entropy: distribution of information across participants of the group 
    \item Representation: How accurately the player's samples reflected their underlying distribution. 

We binned the data into one of 5 bins on each axis, creating a range of difficulty in a 5x5x5 three-dimensional space.
\end{itemize}
We then selected for at least 4 games in each pairwise combination of difficulty bins. We ensured as many bins were filled as possible by re-generating 4-player draws targeting rarer combinations.

\subsubsection{Participants}

We recruited an additional $N=1308$ participants from Prolific and assigned them to $N=327$ unique networks of size four, with $N=2-10$ networks per combination of information condition and communication modality when binned into a 5x5 grid with bins based on the initial 100,000 samples. We excluded any game with more than one participant that did not provide data for two or more rounds. This exclusion criteria was deemed necessary due to the small amount of samples in each combination and the high drop-out rate.  %still doesn't look great tbh I can totally see a reviewer being like "preregister and do it again" 
%I should probably look at which games got zero successful plays out of the 4 per

\subsection{Results}
\subsubsection{Error decreases with interactive communication preforming better in all time periods}

As in the previous rounds of experimentation, we hypothesized that error would reduce over time.We tested this hypothesis using a linear mixed-effects model predicting each slider rating (0 to 100), including fixed effects of round index (continuous; 1 to 9).
We included intercepts and random effects of round index for each player and game. This finding held up to the first experiment round  $b=-1.07$, $t(175.3)=-9.46$, $p<0.001$.
%model2 <- lmer((error) ~ idx + (1+idx | gameID/playerID), data = error_data)

Additionally, we compared to a mixed-effects model including main effects of round index against a model including both condition and sample size. The model with an interaction provided a  better fit in a nested model comparison$\chi^2(1)=22.11$, $p<0.001$, showing that the relationship between collective error over time depended on the communication medium, and had a stronger impact in this round of experiments. %can I say that just based on the significance
% lmer((error) ~ idx + (1+idx | gameID/playerID), data = error_data)
%model4 <- lmer((error) ~ idx + condition + (1+idx |gameID/playerID), data = error_data)
%anova(model2, model4)

As seen in previous work, we also found a tendency for magnet effects. Players put '50', '0', or '100' into the slider more often than other values, suggesting confusion. The slider condition had a slightly greater tendency towards magnet values. %Made a little glm for this but idk how to include it
%Players did NOT put in 25 as a regular magent guess, so our choice of 75 was justifiable and not rigging the system

\subsubsection{Text based communication improves outcomes in more difficult communication conditions }

We split the three axes of information quality down the middle in order to more closely examine the relationship between them. This created two groups for each condition - the "easy" group and the 'hard' group. For each, the 'easy' group would be the conditions where the individual sample was closer to the underlying distribution. We then clustered the networks by the characteristics of their information condition. 

For example, 'Easy Critters', 'Hard Entropy', 'Hard Representation' had a large amount of information across the group (30-40 Critters), but it was distributed unfairly (one player got the majority of the data), and individual samples were not representative of the underlying distribution (one player got all the rabbits, everyone else got some of the squirrels).

We found that the most difficult games (Hard on all conditions) showed a significant benefit from the text-based communication condition when fit to a linear mixed-effects model examining error from group and treatment interaction, $b=-4.48$, $t(1208.1)=-2.98$, $p=0.003$. We included intercepts and random effects of round index for each player and game. This finding held even when excluding the common magnet values of 0, 50, and 100. % lmerTest::lmer(error ~ group *treatment                   + (1 | playerID/gameID),        data = .,        contrasts = list(treatment = contr.sum(2))) %>%   summary()
Similarly, when fit to a linear mixed-effects model examining error from group and treatment interaction intercepts and random effects of round index for each player and game, some treatments with two hard conditions did better with access to text-based communication: hard entropy/representation $b=11.38$, $t(291.5)=6.42$, $p<0.001$, and hard critters/representation $b=6.17$, $t(290.7)=3.23$, $p=0.001$. Hard entropy/hard critters did not show significant difference between slider and interactive communication conditions $b=-0.02$, $t(291.2)=-0.01$, $p=0.995$. 

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.5]{placeholder_figures/hard.png}

    \caption{.}
    \label{fig:channel}
\end{figure*}
\subsubsection{text-based communication alleviated impact of unrepresentative data    }

We decided to examine the effect of data representativeness more closely.  %[fig] 
We ran a random-intercept linear mixed-effects model with fully crossed fixed effects to examine the interactions of the conditions in the raw (ungrouped) data.
We found that in interaction with treatment, representation was highly significant for text-based communication preforming better: $b=4.10$, $t(584.2)=3.48$,$p<0.001$. This finding held even when excluding the common magnet values of 0, 50, and 100.
%d.all_minmax %>%   lmerTest::lmer(    formula = confidence ~  poly(idx, 2) *treatment * entropy_minmax * representation_minmax * nCrittersGame_minmax *error + (1 | gameID),     data = .,   contrasts = list(     treatment             = contr.sum      entropy_minmax        = contr.sum,      representation_minmax = contr.sum,    nCrittersGame_minmax  = contr.sum

\subsubsection{text-based communication alleviated impact of insufficient data    }

We decided to examine the effect of data quantity more closely.  %[fig] 
We ran a random-intercept linear mixed-effects model with fully crossed fixed effects to examine the interactions of the conditions in the raw (ungrouped) data.
We found that in interaction with treatment, critter count was also significant for text-based communication preforming better in harder conditions  $b=-3.55$, $t(584.2)=-3.01$, $p=0.003$. It also improved slider performance in easier conditions. 
% We also found an interaction between entropy and representation, independent of treatment - $b=-3.58$, $t(584.2)=-3.04$, $p=0.002$
We also examined entropy, and found it only had a minor significant effect $b=-2.30$, $t(584.2)=-1.96$, $p=0.051$.
\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.5]{placeholder_figures/000042.png}

   \includegraphics[scale=0.5]{placeholder_figures/image-4.png}    \caption{.}
    \label{fig:channel}
\end{figure*}

%\subsubsection{Individuals with unrepresentative information adjust more significantly to group consensus  }
% removed because it's basically just representation again but it's all filled out
%As in the previous experiment, we found a significant interaction between outlier degree and round index on absolute error, $b=-1.81$, $t(2311.7)=-10.24$, $p<0.001$.  %d.MLE %>%  lmer(abs(error) ~ scale(outlierPercent) * scale(idx) + (1 + scale(outlierPercent) + scale(idx) | gameID),       data = .) %>%   summary() reported scale(outlierPercent):scale(idx)
%Participants who happened to receive outlier samples (e.g. 2 rabbits and 1 squirrel in the condition where squirrels were more likely) initially had higher error in their estimates (light blue line) but the slope of this relationship significantly decreased over time (darker blue lines), indicating that these participants were revising their guesses without other participants' estimates getting significantly worse. 
%\begin{figure*}[t]
%    \centering
    %\includegraphics[scale=0.5]{placeholder_figures/mleError.png}

  %  \caption{.}
 %   \label{fig:channel}
%\end{figure*}
\subsubsection{the content of the messages showed a small benefit to group error rate   }
We decided to examine the contents of the messages being received. We coded it based on the information being transmitted, on two factors - whose information it was (social or personal) and the content of the information (numerical or a belief summary (eg. "50\%").  We fit two separate generalized linear models to examine the relationship between the change in error over time and the frequency of each type of information, examining content and source separately. We found a slightly more significant decrease in error $b=-0.20$, $t=-3.81$, $p<0.001$ with increase in social information per individual. %glm_modps <- glm(  error_change ~ p_count+s_count,  data = error_change_view,  family = gaussian())

When looking at the log of the average error across players in the final round, we found that more communicated social information resulted in less error $b=-0.01$, $t=-4.11$, $p<0.001$ when comparing linear models.  %summary_data_s  %>% lm(log(avg_error) ~ s_count+p_count,       data = .) %>% summary()

When examining the content and source in combination, we found a small effect of social beliefs specifically - $b=-0.02$, $t=-2.87$, $p=0.005$. %lm(formula = log(avg_error) ~ sn_count + sb_count + pb_count +    pn_count, data = .)

\subsubsection{the content of the messages showed a small benefit to group error rate   }
As an exploratory examination, we merged the text data of both experiments. We included both unidirectional and chat from experiment 1, and chat from experiment 2. A generalized linear model maintained the correlation between social belief and final error reduction seen previously, $b=-0.12$, $t=-2.87$, $p=0.005$. %glm(formula = avg_error ~ sb_count + pn_count + sn_count + pb_count, data = summary_data_3_2)

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.5]{placeholder_figures/00002e.png}

    \caption{.}
    \label{fig:channel}
\end{figure*}


\section{Experiment 3: Model}
\subsubsection{degroot}  

\subsubsection{count model}  


\section{Discussion}

Agent-based models of opinion dynamics are popular for a reason. 
They provide a simple interface for examining emergent behavior at the level of populations by grounding it in simple assumptions at the level of individuals. 
However, as these models grow in complexity, it becomes more critical to take a step away from idealized qualitative phenomena toward quantitative measurements of real human groups in controlled settings.
In this paper, we considered a distributed inference task where a small social network collectively inferred a latent probability from limited individual observations. 
We found that groups were able to aggregate knowledge through communicating and successfully reduced the error of their estimates over repeated interactions. 
Critically, we manipulated the \emph{communication modality} they were given to exchange information, finding that a slider interface that enforces standard numerical assumptions of agent-based models was not as effective as natural-language interfaces.

One particularly surprising finding was that groups using a bi-directional messaging interface performed more poorly than groups using a uni-directional messaging interface despite allowing for higher-bandwidth deliberation. 
There are several potential explanations for this counterintuitive result. 
%with the caveat that this is also the messaging system our users were trained on for the UI. 
For one, the additional quantity of messages being exchanged may increase noise or working memory load, outweighing the benefits of bidirectional information flow. 
A related factor is the possibility of introducing interference between different conversations with different interlocutors, leading some information to be more socially salient or weighted more strongly than others; introducing stronger social cues and dissociable avatars might help to reduce interference. 
Finally, these unstructured dialogues may have introduced opportunities for disorganized, ``stream-of-consciousness'' arguments or mistrust about their interlocutor's reliability, undermining cooperative inference in the relatively short window of time allowed for discussion. 
Untangling these possibilities will require analysis of the language itself. But it is clear that simply allowing freer exchange does not automatically improve collective intelligence. 

% Our study differs in on several fronts from the literature on collective inference and belief-updating. 
% First, we measured human behavioural data in a simple statistical inference paradigm which can be used to more finely disassemble and examine the processes of human knowledge optimization and use in varied large scale social contexts, and how this information is transferred through the medium of language.  It does this in a robust and self-contained way. For instance, we found in our data support for several existing theories: In the inital experiment on collective inference, we showed that people do update internal belief models in response to social information, as established by existing work, and this replicates on online social networks. 
% We also showed that they correct from error more significantly when faced with the prospect of their personal knowledge being significantly off from the group belief.

Another puzzling result is that, while error decreased in relative terms, participants also seemed to consistently over- or under-shoot the true latent value in absolute terms, resulting in unexplained residual error. 
It is possible that 9 rounds of interaction is not sufficient for convergence, or that participants generally have a positive response bias in their slider use. 
Another more intriguing explanation is that pieces of redundant social information were being double-counted.
For example, participants could think they were being paired with another new partner on the 4th trial when in fact they were paired with their first partner again \cite{whalen2018sensitivity}.
These misunderstandings could create positive feedback loops that lead networks away from the ground truth.

Overall, our findings highlight the complex relationship between social interaction mechanics and collective intelligence. 
Linguistic coordination decouples observable communication from internal belief updating, enabling indirect inference strategies beyond basic transmission. In other words, the actual content of messages may not cleanly reflect the kinds of scalar opinions transmitted in agent-based models. 
Natural language facilitates a richer space of strategies than simply averaging opinions across neighbors. 
We argue that formal models must move beyond direct transmission assumptions to capture this hidden layer of linguistic coordination. 
By combining controlled behavioral experiments with computational modeling, we can reverse engineer the cognitive mechanisms that enable collectively intelligent behavior to emerge from local exchanges. 
More broadly, our work contributes to a cross-disciplinary understanding of how low-level linguistic processes scale up to shape the dynamics of collective reasoning and collective behavior.


\section{Appendix}




%\bibliographystyle{apacite}
\bibliographystyle{splncs04}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{refs}

\end{document}
